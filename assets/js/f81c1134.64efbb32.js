"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"MNIST-autoencoder","metadata":{"permalink":"/blog/MNIST-autoencoder","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-09-06-autoencoder.md","source":"@site/blog/2024-09-06-autoencoder.md","title":"MNIST Autoencoder","description":"In this post, I\u2019ll explain how I built an autoencoder to compress and reconstruct images from the MNIST dataset. Autoencoders are a type of neural network that learn to compress data into a smaller latent space and reconstruct the original data from that compressed representation. This post will take you through the key steps in the process of training the autoencoder, explaining the important concepts along the way.","date":"2024-09-06T00:00:00.000Z","tags":[{"inline":true,"label":"autoencoder","permalink":"/blog/tags/autoencoder"},{"inline":true,"label":"MNIST","permalink":"/blog/tags/mnist"}],"readingTime":8.78,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"MNIST-autoencoder","title":"MNIST Autoencoder","tags":["autoencoder","MNIST"]},"unlisted":false,"nextItem":{"title":"Adding Learner","permalink":"/blog/adding-learner"}},"content":"In this post, I\u2019ll explain how I built an autoencoder to compress and reconstruct images from the MNIST dataset. Autoencoders are a type of neural network that learn to compress data into a smaller latent space and reconstruct the original data from that compressed representation. This post will take you through the key steps in the process of training the autoencoder, explaining the important concepts along the way.\\n\x3c!--truncate--\x3e\\n\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torchvision import datasets, transforms\\nfrom torch.utils.data import DataLoader\\nimport matplotlib.pyplot as plt\\nimport torchvision\\nimport numpy as np\\nimport os\\n\\n# Set seed for reproducibility\\nseed = 42\\ntorch.manual_seed(seed)\\n\\n# Use GPU if available\\ndevice = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\nprint(\\"Using device:\\", device)\\n```\\n\\n    Using device: cuda\\n\\n\\n### Data\\n\\nThe MNIST dataset contains 28x28 pixel images of handwritten digits (0-9). Each image has 784(28x28) pixels. In this project, I used separate datasets for training and validation. I didn\u2019t hold out a testing set, as this was more of an experiment to understand the autoencoder\'s functionality.\\n\\n\\n```python\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),  # Scales pixel values between 0 and 1\\n])\\n\\n# Download MNIST dataset\\ntrain_dataset = datasets.MNIST(root=\'./data\', train=True, transform=transform, download=True)\\nvalid_dataset = datasets.MNIST(root=\'./data\', train=False, transform=transform, download=True)\\n\\n# Load data\\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\\nvalid_loader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=False)\\n```\\n\\n## Model Architecture\\n\\n I experimented with a few different architectures, but I ended up with an autoencoder that compresses the input image (784 pixels) down to a 6-dimensional latent space. The architecture consists of an encoder, which progressively reduces the data through several layers, and a decoder, which mirrors the encoder to reconstruct the image.\\n\\nHere\u2019s a visual representation of the architecture:\\n\\n<svg xmlns=\\"http://www.w3.org/2000/svg\\" viewBox=\\"0 0 1000 320\\">\\n  \x3c!-- Input Layer --\x3e\\n  <rect x=\\"10\\" y=\\"100\\" width=\\"180\\" height=\\"100\\" fill=\\"#1e3a8a\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"100\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">Input Image (784)</text>\\n  \\n  \x3c!-- Encoder Layers --\x3e\\n  <rect x=\\"210\\" y=\\"112\\" width=\\"80\\" height=\\"76\\" fill=\\"#065f46\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"250\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">128</text>\\n  \\n  <rect x=\\"310\\" y=\\"120\\" width=\\"60\\" height=\\"60\\" fill=\\"#065f46\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"340\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">64</text>\\n  \\n  <rect x=\\"390\\" y=\\"134\\" width=\\"40\\" height=\\"32\\" fill=\\"#065f46\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"410\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">12</text>\\n  \\n  \x3c!-- Latent Space --\x3e\\n  <rect x=\\"450\\" y=\\"143\\" width=\\"30\\" height=\\"14\\" fill=\\"#854d0e\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"465\\" y=\\"205\\" text-anchor=\\"middle\\" font-size=\\"16\\" fill=\\"#ffffff\\">Latent (6)</text>\\n  \\n  \x3c!-- Decoder Layers --\x3e\\n  <rect x=\\"500\\" y=\\"134\\" width=\\"40\\" height=\\"32\\" fill=\\"#9f1239\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"520\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">12</text>\\n  \\n  <rect x=\\"560\\" y=\\"120\\" width=\\"60\\" height=\\"60\\" fill=\\"#9f1239\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"590\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">64</text>\\n  \\n  <rect x=\\"640\\" y=\\"112\\" width=\\"80\\" height=\\"76\\" fill=\\"#9f1239\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"680\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">128</text>\\n  \\n  \x3c!-- Output Layer --\x3e\\n  <rect x=\\"740\\" y=\\"100\\" width=\\"180\\" height=\\"100\\" fill=\\"#1e3a8a\\" stroke=\\"#ffffff\\" />\\n  <text x=\\"830\\" y=\\"155\\" text-anchor=\\"middle\\" font-size=\\"14\\" fill=\\"#ffffff\\">Output Image (784)</text>\\n  \\n  \x3c!-- Connecting Lines --\x3e\\n  <line x1=\\"190\\" y1=\\"150\\" x2=\\"210\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  <line x1=\\"290\\" y1=\\"150\\" x2=\\"310\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  <line x1=\\"370\\" y1=\\"150\\" x2=\\"390\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  <line x1=\\"430\\" y1=\\"150\\" x2=\\"450\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  <line x1=\\"480\\" y1=\\"150\\" x2=\\"500\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  <line x1=\\"540\\" y1=\\"150\\" x2=\\"560\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  <line x1=\\"620\\" y1=\\"150\\" x2=\\"640\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  <line x1=\\"720\\" y1=\\"150\\" x2=\\"740\\" y2=\\"150\\" stroke=\\"#ffffff\\" stroke-width=\\"2\\" />\\n  \\n  \x3c!-- Labels --\x3e\\n  <text x=\\"300\\" y=\\"230\\" text-anchor=\\"middle\\" font-size=\\"16\\" fill=\\"#ffffff\\">Encoder</text>\\n  <text x=\\"630\\" y=\\"230\\" text-anchor=\\"middle\\" font-size=\\"16\\" fill=\\"#ffffff\\">Decoder</text>\\n</svg>\\n\\n\\n```python\\n# Define the model\\nclass Autoencoder(nn.Module):\\n    def __init__(self):\\n        super(Autoencoder, self).__init__()\\n        # Encoder\\n        self.encoder = nn.Sequential(\\n            nn.Linear(28*28, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, 12),\\n            nn.ReLU(),\\n            nn.Linear(12, 6)  # Latent representation\\n        )\\n        # Decoder\\n        self.decoder = nn.Sequential(\\n            nn.Linear(6, 12),\\n            nn.ReLU(),\\n            nn.Linear(12, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 28*28),\\n            nn.Sigmoid()\\n        )\\n\\n    def forward(self, x):\\n        x = self.encoder(x)\\n        x = self.decoder(x)\\n        return x\\n    \\n    def encode(self, x):\\n        return self.encoder(x)\\n\\n```\\n\\nOne variation I tried was scaling the input data between [-1, 1] instead of [0, 1]. I also changed the output activation function from Sigmoid to hyperbolic tangent (tanh). Sigmoid maps outputs to the [0, 1] range, while tanh maps them to [-1, 1]. Having the input and output ranges match helped the model learn faster. I\'ll plot the two activation functions below to show the difference.\\n\\nIn the end, I didn\u2019t notice any change in performance, so I reverted to scaling the data between [0, 1], which is more common.\\n\\n\\n```python\\ndef plot_sigmoid_tanh():\\n    # range of x values\\n    x = np.linspace(-6, 6, 100)\\n\\n    # functions to plot\\n    sigmoid = 1 / (1 + np.exp(-x))\\n    tanh = np.tanh(x)\\n\\n    plt.figure(figsize=(8, 5))\\n    plt.plot(x, sigmoid, label=\'Sigmoid\', color=\'blue\')\\n    plt.plot(x, tanh, label=\'Tanh\', color=\'red\')\\n    plt.title(\'Sigmoid vs Tanh Activation Functions\')\\n    plt.xlabel(\'Input\')\\n    plt.ylabel(\'Activation\')\\n    plt.grid(True)\\n    plt.legend()\\n    plt.show()\\n\\n# Plot the Sigmoid and Tanh functions\\nplot_sigmoid_tanh()\\n```\\n\\n\\n    \\n![png](2024-09-06/03a%20Autoencoder-linear_files/03a%20Autoencoder-linear_7_0.png)\\n    \\n\\n\\n### Training the Model\\n\\nI used Mean Squared Error (MSE) as the loss function and the Adam optimizer. The model was trained for 50 epochs.\\n\\n\\n```python\\ndef train(model, dataloader, device):\\n    model.train()\\n    total_loss = 0\\n    for images, _ in dataloader:\\n        images = images.view(images.size(0), -1).to(device)\\n        optimizer.zero_grad()\\n        outputs = model(images)\\n        loss = criterion(outputs, images)\\n        loss.backward()\\n        optimizer.step()\\n        total_loss += loss.item()\\n    average_loss = total_loss / len(dataloader)\\n    return average_loss\\n\\ndef validate(model, dataloader, device):\\n    model.eval()\\n    total_loss = 0\\n    with torch.no_grad():\\n        for images, _ in dataloader:\\n            images = images.view(images.size(0), -1).to(device)\\n            outputs = model(images)\\n            loss = criterion(outputs, images)\\n            total_loss += loss.item()\\n    average_loss = total_loss / len(dataloader)\\n    return average_loss\\n\\n\\n# Create an instance of the model and move it to the device\\nautoencoder = Autoencoder().to(device)\\n\\n# Define the loss function and optimizer\\ncriterion = nn.MSELoss()\\noptimizer = optim.Adam(autoencoder.parameters(), lr=0.0005)\\n\\n# Training Loop\\ntrain_losses = []\\nvalid_losses = []\\nnum_epochs = 50\\nfor epoch in range(num_epochs):\\n    train_loss = train(autoencoder, train_loader, device)\\n    valid_loss = validate(autoencoder, valid_loader, device)\\n    train_losses.append(train_loss)\\n    valid_losses.append(valid_loss)\\n```\\n\\n### Training and validation loss\\n\\nHere is a plot of the training and validation loss. It shows that the model is learning and not overfitting.\\n\\n\\n```python\\nplt.figure(figsize=(10, 5))\\nplt.plot(train_losses, label=\'Training Loss\')\\nplt.plot(valid_losses, label=\'Validation Loss\')\\nplt.title(\'Training and Validation Loss\')\\nplt.xlabel(\'Epochs\')\\nplt.ylabel(\'Loss\')\\nplt.legend()\\nplt.show()\\n```\\n\\n\\n    \\n![png](2024-09-06/03a%20Autoencoder-linear_files/03a%20Autoencoder-linear_11_0.png)\\n    \\n\\n\\nZooming in on the last 15 epocs we can see that training and validation are still decreasing although the rate of decrease has slowed down.\\n\\n\\n```python\\nplt.figure(figsize=(10, 5))\\nplt.plot(train_losses[-15:], label=\'Training Loss\')\\nplt.plot(valid_losses[-15:], label=\'Validation Loss\')\\nplt.title(\'Training and Validation Loss - Last 15 Epochs\')\\nplt.xlabel(\'Epochs\')\\nplt.ylabel(\'Loss\')\\nplt.legend()\\nplt.show()\\n```\\n\\n\\n    \\n![png](2024-09-06/03a%20Autoencoder-linear_files/03a%20Autoencoder-linear_13_0.png)\\n    \\n\\n\\n### Results\\n\\nAfter training, I tested the autoencoder by passing images from the validation set and comparing the original and reconstructed images. Here\u2019s a visual comparison.\\n\\n\\n```python\\ndef imshow(img):\\n    npimg = img.numpy()\\n    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\'gray\')\\n    plt.show()\\n\\n# Display original images\\nimages, labels = next(iter(valid_loader))\\nimages = images.to(device)\\nprint(\\"Original Images\\")\\nimshow(torchvision.utils.make_grid(images[:5].cpu()))\\n\\n# Display reconstructed images\\nimages_flattened = images.view(images.size(0), -1)\\noutputs = autoencoder(images_flattened)\\noutputs = outputs.view(outputs.size(0), 1, 28, 28) # Reshape the outputs\\nprint(\\"Reconstructed Images\\")\\nimshow(torchvision.utils.make_grid(outputs[:5].cpu()))\\n```\\n\\n    Original Images\\n\\n\\n\\n    \\n![png](2024-09-06/03a%20Autoencoder-linear_files/03a%20Autoencoder-linear_15_1.png)\\n    \\n\\n\\n    Reconstructed Images\\n\\n\\n\\n    \\n![png](2024-09-06/03a%20Autoencoder-linear_files/03a%20Autoencoder-linear_15_3.png)\\n    \\n\\n\\n### Saving to disk and restoring\\n\\nThis isn\'t lossless compression, however this is really good results given that the latent space representation is only six numbers!\\n\\nIn the code below I use the encoder and decoder separately to save and load the latent space representation to confirm everything is working as expected.\\n\\nThis was interesting and took a couple tries. At first I was just serializing the tensor to disk, however the tensor has metadata in it that inflated the size to around 2,000 bytes. I was able to remove the metadata below by detaching and converting to a numpy array.\\n\\n\\n```python\\n# Create a latent representation of the first image in the validation set\\nimages, _ = next(iter(valid_loader))\\nimages = images.view(images.size(0), -1).to(device)\\nlatent_representation = autoencoder.encode(images[:1])\\n\\n# Convert to numpy array to avoid saving tensor metadata to disk\\nlatent_numpy = latent_representation.cpu().detach().numpy()\\n\\n# Save to disk using a binary format\\nlatent_numpy.tofile(\\"latent_representation.bin\\")\\n\\n# Print the size of the saved file\\nfile_size = os.path.getsize(\\"latent_representation.bin\\")\\nprint(f\\"Size of the latent representations on disk: {file_size} bytes\\")\\n```\\n\\n    Size of the latent representations on disk: 24 bytes\\n\\n\\nThe compressed image ends up being twenty four bytes. That is our expected size because the latent representation is six floats at four bytes each.\\n\\nThe original input was 784(28x28) numbers. Each of those numbers would fit into a byte because they were in the range 0-255. That means that we got a 97% size reduction!\\n\\nLet\'s reconstruct the saved image to confirm that the autoencoder is working as expected.\\n\\n\\n```python\\n# Display the original image\\noriginal_image = images[0].view(1, 28, 28).cpu()\\nprint(\\"Original Image\\")\\nimshow(torchvision.utils.make_grid(original_image))\\n\\n# Load from disk\\nlatent_size = 6\\nlatent_representation = np.fromfile(\\"latent_representation.bin\\", dtype=np.float32, count=latent_size)\\nlatent_representation = torch.from_numpy(latent_representation).to(device).unsqueeze(0)\\n\\n# Use the decoder to reconstruct the image\\nreconstructed = autoencoder.decoder(latent_representation)\\n\\n# Display the reconstructed image\\nreconstructed_image = reconstructed.view(1, 28, 28).cpu()\\nprint(\\"Reconstructed Image\\")\\nimshow(torchvision.utils.make_grid(reconstructed_image))\\n```\\n\\n    Original Image\\n\\n\\n\\n    \\n![png](2024-09-06/03a%20Autoencoder-linear_files/03a%20Autoencoder-linear_20_1.png)\\n    \\n\\n\\n    Reconstructed Image\\n\\n\\n\\n    \\n![png](2024-09-06/03a%20Autoencoder-linear_files/03a%20Autoencoder-linear_20_3.png)\\n    \\n\\n\\n### Conclusion\\n\\nIn this project, I built an autoencoder to compress and reconstruct MNIST images. The autoencoder achieved about 97% compression by reducing the input from 784 pixels to a 6-number latent space. This type of model has potential applications in data compression, anomaly detection, and data denoising. While this was a simple experiment, autoencoders are a powerful tool in many areas of machine learning."},{"id":"adding-learner","metadata":{"permalink":"/blog/adding-learner","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-07-30-adding-learner.md","source":"@site/blog/2024-07-30-adding-learner.md","title":"Adding Learner","description":"In this post, I\'m going to describe building a simple neural network using PyTorch that learns to add two numbers.","date":"2024-07-30T00:00:00.000Z","tags":[{"inline":true,"label":"math-learner","permalink":"/blog/tags/math-learner"}],"readingTime":3.91,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"adding-learner","title":"Adding Learner","tags":["math-learner"]},"unlisted":false,"prevItem":{"title":"MNIST Autoencoder","permalink":"/blog/MNIST-autoencoder"}},"content":"In this post, I\'m going to describe building a simple neural network using PyTorch that learns to add two numbers.\\n\\n### Data Generation\\n\\nFirst, I\'ll define a method that generates the data. This method returns two tensors. The first tensor, `x`, is the input to the model, and the second tensor, `y`, is the expected output (the sum of the pairs of numbers).\\n\\n```python\\ndef generate_data(num_samples=1000):\\n    x = torch.randint(0, 100, (num_samples, 2), dtype=torch.float32)\\n    y = torch.sum(x, dim=1, keepdim=True)\\n    return x, y\\n```\\n\x3c!--truncate--\x3e\\n\\n### Defining the Model\\n\\nNext, I\'ll define a simple neural network module. This module has two linear layers with a ReLU activation function between them. The first layer takes two inputs, and the second layer outputs one value.\\n\\n```python\\nclass SimpleAdder(nn.Module):\\n    def __init__(self):\\n        super(SimpleAdder, self).__init__()\\n        self.fc1 = nn.Linear(2, 10)\\n        self.fc2 = nn.Linear(10, 1)\\n    \\n    def forward(self, x):\\n        x = torch.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n```\\n\\n### Training the Model\\n\\nHere\'s the method to train the model. I save the training and validation losses so that I can see it get better and look for any issues like overfitting.\\n\\n```python\\ndef train_model_with_validation(model, criterion, optimizer, x_train, y_train, x_val, y_val, num_epochs):\\n    train_losses = []\\n    val_losses = []\\n    \\n    for epoch in range(num_epochs):\\n        # Training\\n        model.train()\\n        outputs = model(x_train)\\n        train_loss = criterion(outputs, y_train)\\n        optimizer.zero_grad()\\n        train_loss.backward()\\n        optimizer.step()\\n        \\n        # Validation\\n        model.eval()\\n        with torch.no_grad():\\n            val_outputs = model(x_val)\\n            val_loss = criterion(val_outputs, y_val)\\n        \\n        # Store losses\\n        train_losses.append(train_loss.item())\\n        val_losses.append(val_loss.item())\\n        \\n        # Print progress every 100 epochs\\n        if (epoch + 1) % 100 == 0:\\n            print(f\'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\')\\n    \\n    return train_losses, val_losses\\n```\\n\\n### Testing the Model\\n\\nI sent in whole numbers (they were floats, however their fractional part was zero) into the model to be added. The model returned a number with a fractional part. This is very useful for training because it gives the loss function a smoother gradient to optimize.\\n\\nWhen computing accuracy after training the model, I will do two things differently. First I\'m going to use new random data. Second I\'m going to round the predictions. That will give me my whole number output.\\n\\n\\n```python\\ndef test_model(model, x_test):\\n    with torch.no_grad():\\n        predicted = model(x_test)\\n        rounded_predicted = torch.round(predicted)\\n    return rounded_predicted\\n\\ndef calculate_accuracy(predictions, targets):\\n    correct = (predictions == targets).sum().item()\\n    total = targets.size(0)\\n    accuracy = correct / total * 100\\n    return correct, total, accuracy\\n```\\n\\n## Main Function\\n\\nHere\'s the main function that puts everything together.\\n\\n```python\\n# Generate training and validation data. I split the random data 80% / 20%.\\nx, y = generate_data(100000)\\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\\n\\n# Initialize the model, criterion, and optimizer\\nmodel = SimpleAdder()\\ncriterion = nn.MSELoss()\\noptimizer = optim.Adam(model.parameters(), lr=0.01)\\n\\n# Train the model with validation\\nnum_epochs = 1000\\ntrain_losses, val_losses = train_model_with_validation(model, criterion, optimizer, x_train, y_train, x_val, y_val, num_epochs)\\n\\n# Plot the loss curves\\nplot_losses(train_losses, val_losses, num_epochs)\\n\\n# Test the model on a new test set\\nx_test, y_test = generate_data(1000)\\ny_pred = test_model(model, x_test)\\n\\n# Calculate accuracy\\ncorrect, total, accuracy = calculate_accuracy(y_pred, y_test)\\n\\n# Print results\\nprint(f\'Predictions:\\\\n{y_pred}\')\\nprint(f\'Actual sums:\\\\n{y_test}\')\\nprint(f\'Number of correct predictions: {correct}/{total}\')\\nprint(f\'Accuracy: {accuracy:.2f}%\')\\n```\\n\\n### Looking at the loss graphs\\n\\nDue to the simple model and dataset I was able to use a very large number of samples and epocs.\\n\\nLooking at all the epocs it looks good.\\n\\n![Loss Graphs](2024-07-30/all-epocs.png)\\n\\nZooming in on the last 100 epocs we see that it was still slightly improving!\\n\\n![Last 100 epocs](2024-07-30/last-100-epocs.png)\\n\\n### Output and Results\\n\\nHere is the output. The neural network shows that it is learning by the decreasing loss values.\\n\\n```\\nEpoch [100/1000], Train Loss: 0.8650, Val Loss: 0.9121\\nEpoch [200/1000], Train Loss: 0.0756, Val Loss: 0.0751\\nEpoch [300/1000], Train Loss: 0.0565, Val Loss: 0.0561\\nEpoch [400/1000], Train Loss: 0.0393, Val Loss: 0.0390\\nEpoch [500/1000], Train Loss: 0.0255, Val Loss: 0.0253\\nEpoch [600/1000], Train Loss: 0.0153, Val Loss: 0.0152\\nEpoch [700/1000], Train Loss: 0.0085, Val Loss: 0.0085\\nEpoch [800/1000], Train Loss: 0.0044, Val Loss: 0.0044\\nEpoch [900/1000], Train Loss: 0.0021, Val Loss: 0.0021\\nEpoch [1000/1000], Train Loss: 0.0009, Val Loss: 0.0009\\n\\n```\\n\\nAfter the model has finished training, here are the results of running it on the test set.\\n\\n```\\nPredictions:\\ntensor([[ 19.],\\n        [113.],\\n        [101.],\\n        [137.],\\n...\\n        [181.],\\n        [ 70.],\\n        [141.],\\n        [ 93.],\\n        [ 86.],\\n        [163.]])\\nActual sums:\\ntensor([[ 19.],\\n        [113.],\\n        [101.],\\n        [137.],\\n...\\n        [181.],\\n        [ 70.],\\n        [141.],\\n        [ 93.],\\n        [ 86.],\\n        [163.]])\\nNumber of correct predictions: 1000/1000\\nAccuracy: 100.00%\\n```\\n\\n### Wrap-up\\n\\nThis simple neural network successfully learned to add two numbers. The final accuracy on the test set is 100%, indicating that the model did a good job at predicting the sum of the test input pairs. This example demonstrates the basic workflow of creating and training a neural network in PyTorch."}]}}')}}]);