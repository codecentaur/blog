"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[477],{10:n=>{n.exports=JSON.parse('{"blogPosts":[{"id":"adding-learner","metadata":{"permalink":"/blog/adding-learner","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-07-30-adding-learner.md","source":"@site/blog/2024-07-30-adding-learner.md","title":"Adding Learner","description":"In this post, I\'m going to describe building a simple neural network using PyTorch that learns to add two numbers.","date":"2024-07-30T00:00:00.000Z","formattedDate":"July 30, 2024","tags":[{"label":"math-learner","permalink":"/blog/tags/math-learner"}],"readingTime":3.875,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"adding-learner","title":"Adding Learner","tags":["math-learner"]},"unlisted":false},"content":"In this post, I\'m going to describe building a simple neural network using PyTorch that learns to add two numbers.\\n\\n### Data Generation\\n\\nFirst, I\'ll define a method that generates the data. This method returns two tensors. The first tensor, `x`, is the input to the model, and the second tensor, `y`, is the expected output (the sum of the pairs of numbers).\\n\\n```python\\ndef generate_data(num_samples=1000):\\n    x = torch.randint(0, 100, (num_samples, 2), dtype=torch.float32)\\n    y = torch.sum(x, dim=1, keepdim=True)\\n    return x, y\\n```\\n\x3c!--truncate--\x3e\\n\\n### Defining the Model\\n\\nNext, I\'ll define a simple neural network module. This module has two linear layers with a ReLU activation function between them. The first layer takes two inputs, and the second layer outputs one value.\\n\\n```python\\nclass SimpleAdder(nn.Module):\\n    def __init__(self):\\n        super(SimpleAdder, self).__init__()\\n        self.fc1 = nn.Linear(2, 10)\\n        self.fc2 = nn.Linear(10, 1)\\n    \\n    def forward(self, x):\\n        x = torch.relu(self.fc1(x))\\n        x = self.fc2(x)\\n        return x\\n```\\n\\n### Training the Model\\n\\nHere\'s the method to train the model. I save the training and validation losses so that I can see it get better and look for any issues like overfitting.\\n\\n```python\\ndef train_model_with_validation(model, criterion, optimizer, x_train, y_train, x_val, y_val, num_epochs):\\n    train_losses = []\\n    val_losses = []\\n    \\n    for epoch in range(num_epochs):\\n        # Training\\n        model.train()\\n        outputs = model(x_train)\\n        train_loss = criterion(outputs, y_train)\\n        optimizer.zero_grad()\\n        train_loss.backward()\\n        optimizer.step()\\n        \\n        # Validation\\n        model.eval()\\n        with torch.no_grad():\\n            val_outputs = model(x_val)\\n            val_loss = criterion(val_outputs, y_val)\\n        \\n        # Store losses\\n        train_losses.append(train_loss.item())\\n        val_losses.append(val_loss.item())\\n        \\n        # Print progress every 100 epochs\\n        if (epoch + 1) % 100 == 0:\\n            print(f\'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\')\\n    \\n    return train_losses, val_losses\\n```\\n\\n### Testing the Model\\n\\nI sent in whole numbers (they were floats, however their fractional part was zero) into the model to be added. The model returned a number with a fractional part. This is very useful for training because it gives the loss function a smoother gradient to optimize.\\n\\nWhen computing accuracy after training the model, I will do two things differently. First I\'m going to use new random data. Second I\'m going to round the predictions. That will give me my whole number output.\\n\\n\\n```python\\ndef test_model(model, x_test):\\n    with torch.no_grad():\\n        predicted = model(x_test)\\n        rounded_predicted = torch.round(predicted)\\n    return rounded_predicted\\n\\ndef calculate_accuracy(predictions, targets):\\n    correct = (predictions == targets).sum().item()\\n    total = targets.size(0)\\n    accuracy = correct / total * 100\\n    return correct, total, accuracy\\n```\\n\\n## Main Function\\n\\nHere\'s the main function that puts everything together.\\n\\n```python\\n# Generate training and validation data. I split the random data 80% / 20%.\\nx, y = generate_data(100000)\\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\\n\\n# Initialize the model, criterion, and optimizer\\nmodel = SimpleAdder()\\ncriterion = nn.MSELoss()\\noptimizer = optim.Adam(model.parameters(), lr=0.01)\\n\\n# Train the model with validation\\nnum_epochs = 1000\\ntrain_losses, val_losses = train_model_with_validation(model, criterion, optimizer, x_train, y_train, x_val, y_val, num_epochs)\\n\\n# Plot the loss curves\\nplot_losses(train_losses, val_losses, num_epochs)\\n\\n# Test the model on a new test set\\nx_test, y_test = generate_data(1000)\\ny_pred = test_model(model, x_test)\\n\\n# Calculate accuracy\\ncorrect, total, accuracy = calculate_accuracy(y_pred, y_test)\\n\\n# Print results\\nprint(f\'Predictions:\\\\n{y_pred}\')\\nprint(f\'Actual sums:\\\\n{y_test}\')\\nprint(f\'Number of correct predictions: {correct}/{total}\')\\nprint(f\'Accuracy: {accuracy:.2f}%\')\\n```\\n\\n### Looking at the loss graphs\\n\\nDue to the simple model and dataset I was able to use a very large number of samples and epocs.\\n\\nLooking at all the epocs it looks good.\\n\\n![Loss Graphs](2024-07-30/all-epocs.png)\\n\\nZooming in on the last 100 epocs we see that it was still slightly improving!\\n\\n![Last 100 epocs](2024-07-30/last-100-epocs.png)\\n\\n### Output and Results\\n\\nHere is the output. The neural network shows that it is learning by the decreasing loss values.\\n\\n```\\nEpoch [100/1000], Train Loss: 0.8650, Val Loss: 0.9121\\nEpoch [200/1000], Train Loss: 0.0756, Val Loss: 0.0751\\nEpoch [300/1000], Train Loss: 0.0565, Val Loss: 0.0561\\nEpoch [400/1000], Train Loss: 0.0393, Val Loss: 0.0390\\nEpoch [500/1000], Train Loss: 0.0255, Val Loss: 0.0253\\nEpoch [600/1000], Train Loss: 0.0153, Val Loss: 0.0152\\nEpoch [700/1000], Train Loss: 0.0085, Val Loss: 0.0085\\nEpoch [800/1000], Train Loss: 0.0044, Val Loss: 0.0044\\nEpoch [900/1000], Train Loss: 0.0021, Val Loss: 0.0021\\nEpoch [1000/1000], Train Loss: 0.0009, Val Loss: 0.0009\\n\\n```\\n\\nAfter the model has finished training, here are the results of running it on the test set.\\n\\n```\\nPredictions:\\ntensor([[ 19.],\\n        [113.],\\n        [101.],\\n        [137.],\\n...\\n        [181.],\\n        [ 70.],\\n        [141.],\\n        [ 93.],\\n        [ 86.],\\n        [163.]])\\nActual sums:\\ntensor([[ 19.],\\n        [113.],\\n        [101.],\\n        [137.],\\n...\\n        [181.],\\n        [ 70.],\\n        [141.],\\n        [ 93.],\\n        [ 86.],\\n        [163.]])\\nNumber of correct predictions: 1000/1000\\nAccuracy: 100.00%\\n```\\n\\n### Wrap-up\\n\\nThis simple neural network successfully learned to add two numbers. The final accuracy on the test set is 100%, indicating that the model did a good job at predicting the sum of the test input pairs. This example demonstrates the basic workflow of creating and training a neural network in PyTorch."}]}')}}]);