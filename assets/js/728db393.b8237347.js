"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[913],{8639:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var s=t(4848),a=t(8453);const r={slug:"adding-learner",title:"Adding Learner",tags:["math-learner"]},i=void 0,o={permalink:"/blog/adding-learner",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-07-30-adding-learner.md",source:"@site/blog/2024-07-30-adding-learner.md",title:"Adding Learner",description:"In this post, I'm going to describe building a simple neural network using PyTorch that learns to add two numbers.",date:"2024-07-30T00:00:00.000Z",tags:[{inline:!0,label:"math-learner",permalink:"/blog/tags/math-learner"}],readingTime:3.875,hasTruncateMarker:!0,authors:[],frontMatter:{slug:"adding-learner",title:"Adding Learner",tags:["math-learner"]},unlisted:!1,prevItem:{title:"MNIST Autoencoder",permalink:"/blog/MNIST-autoencoder"}},l={authorsImageUrls:[]},d=[{value:"Data Generation",id:"data-generation",level:3},{value:"Defining the Model",id:"defining-the-model",level:3},{value:"Training the Model",id:"training-the-model",level:3},{value:"Testing the Model",id:"testing-the-model",level:3},{value:"Main Function",id:"main-function",level:2},{value:"Looking at the loss graphs",id:"looking-at-the-loss-graphs",level:3},{value:"Output and Results",id:"output-and-results",level:3},{value:"Wrap-up",id:"wrap-up",level:3}];function c(e){const n={code:"code",h2:"h2",h3:"h3",img:"img",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"In this post, I'm going to describe building a simple neural network using PyTorch that learns to add two numbers."}),"\n",(0,s.jsx)(n.h3,{id:"data-generation",children:"Data Generation"}),"\n",(0,s.jsxs)(n.p,{children:["First, I'll define a method that generates the data. This method returns two tensors. The first tensor, ",(0,s.jsx)(n.code,{children:"x"}),", is the input to the model, and the second tensor, ",(0,s.jsx)(n.code,{children:"y"}),", is the expected output (the sum of the pairs of numbers)."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def generate_data(num_samples=1000):\n    x = torch.randint(0, 100, (num_samples, 2), dtype=torch.float32)\n    y = torch.sum(x, dim=1, keepdim=True)\n    return x, y\n"})}),"\n",(0,s.jsx)(n.h3,{id:"defining-the-model",children:"Defining the Model"}),"\n",(0,s.jsx)(n.p,{children:"Next, I'll define a simple neural network module. This module has two linear layers with a ReLU activation function between them. The first layer takes two inputs, and the second layer outputs one value."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class SimpleAdder(nn.Module):\n    def __init__(self):\n        super(SimpleAdder, self).__init__()\n        self.fc1 = nn.Linear(2, 10)\n        self.fc2 = nn.Linear(10, 1)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n"})}),"\n",(0,s.jsx)(n.h3,{id:"training-the-model",children:"Training the Model"}),"\n",(0,s.jsx)(n.p,{children:"Here's the method to train the model. I save the training and validation losses so that I can see it get better and look for any issues like overfitting."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def train_model_with_validation(model, criterion, optimizer, x_train, y_train, x_val, y_val, num_epochs):\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        outputs = model(x_train)\n        train_loss = criterion(outputs, y_train)\n        optimizer.zero_grad()\n        train_loss.backward()\n        optimizer.step()\n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_outputs = model(x_val)\n            val_loss = criterion(val_outputs, y_val)\n        \n        # Store losses\n        train_losses.append(train_loss.item())\n        val_losses.append(val_loss.item())\n        \n        # Print progress every 100 epochs\n        if (epoch + 1) % 100 == 0:\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n    \n    return train_losses, val_losses\n"})}),"\n",(0,s.jsx)(n.h3,{id:"testing-the-model",children:"Testing the Model"}),"\n",(0,s.jsx)(n.p,{children:"I sent in whole numbers (they were floats, however their fractional part was zero) into the model to be added. The model returned a number with a fractional part. This is very useful for training because it gives the loss function a smoother gradient to optimize."}),"\n",(0,s.jsx)(n.p,{children:"When computing accuracy after training the model, I will do two things differently. First I'm going to use new random data. Second I'm going to round the predictions. That will give me my whole number output."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def test_model(model, x_test):\n    with torch.no_grad():\n        predicted = model(x_test)\n        rounded_predicted = torch.round(predicted)\n    return rounded_predicted\n\ndef calculate_accuracy(predictions, targets):\n    correct = (predictions == targets).sum().item()\n    total = targets.size(0)\n    accuracy = correct / total * 100\n    return correct, total, accuracy\n"})}),"\n",(0,s.jsx)(n.h2,{id:"main-function",children:"Main Function"}),"\n",(0,s.jsx)(n.p,{children:"Here's the main function that puts everything together."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Generate training and validation data. I split the random data 80% / 20%.\nx, y = generate_data(100000)\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Initialize the model, criterion, and optimizer\nmodel = SimpleAdder()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Train the model with validation\nnum_epochs = 1000\ntrain_losses, val_losses = train_model_with_validation(model, criterion, optimizer, x_train, y_train, x_val, y_val, num_epochs)\n\n# Plot the loss curves\nplot_losses(train_losses, val_losses, num_epochs)\n\n# Test the model on a new test set\nx_test, y_test = generate_data(1000)\ny_pred = test_model(model, x_test)\n\n# Calculate accuracy\ncorrect, total, accuracy = calculate_accuracy(y_pred, y_test)\n\n# Print results\nprint(f'Predictions:\\n{y_pred}')\nprint(f'Actual sums:\\n{y_test}')\nprint(f'Number of correct predictions: {correct}/{total}')\nprint(f'Accuracy: {accuracy:.2f}%')\n"})}),"\n",(0,s.jsx)(n.h3,{id:"looking-at-the-loss-graphs",children:"Looking at the loss graphs"}),"\n",(0,s.jsx)(n.p,{children:"Due to the simple model and dataset I was able to use a very large number of samples and epocs."}),"\n",(0,s.jsx)(n.p,{children:"Looking at all the epocs it looks good."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Loss Graphs",src:t(6692).A+"",width:"589",height:"455"})}),"\n",(0,s.jsx)(n.p,{children:"Zooming in on the last 100 epocs we see that it was still slightly improving!"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Last 100 epocs",src:t(2721).A+"",width:"593",height:"455"})}),"\n",(0,s.jsx)(n.h3,{id:"output-and-results",children:"Output and Results"}),"\n",(0,s.jsx)(n.p,{children:"Here is the output. The neural network shows that it is learning by the decreasing loss values."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Epoch [100/1000], Train Loss: 0.8650, Val Loss: 0.9121\nEpoch [200/1000], Train Loss: 0.0756, Val Loss: 0.0751\nEpoch [300/1000], Train Loss: 0.0565, Val Loss: 0.0561\nEpoch [400/1000], Train Loss: 0.0393, Val Loss: 0.0390\nEpoch [500/1000], Train Loss: 0.0255, Val Loss: 0.0253\nEpoch [600/1000], Train Loss: 0.0153, Val Loss: 0.0152\nEpoch [700/1000], Train Loss: 0.0085, Val Loss: 0.0085\nEpoch [800/1000], Train Loss: 0.0044, Val Loss: 0.0044\nEpoch [900/1000], Train Loss: 0.0021, Val Loss: 0.0021\nEpoch [1000/1000], Train Loss: 0.0009, Val Loss: 0.0009\n\n"})}),"\n",(0,s.jsx)(n.p,{children:"After the model has finished training, here are the results of running it on the test set."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Predictions:\ntensor([[ 19.],\n        [113.],\n        [101.],\n        [137.],\n...\n        [181.],\n        [ 70.],\n        [141.],\n        [ 93.],\n        [ 86.],\n        [163.]])\nActual sums:\ntensor([[ 19.],\n        [113.],\n        [101.],\n        [137.],\n...\n        [181.],\n        [ 70.],\n        [141.],\n        [ 93.],\n        [ 86.],\n        [163.]])\nNumber of correct predictions: 1000/1000\nAccuracy: 100.00%\n"})}),"\n",(0,s.jsx)(n.h3,{id:"wrap-up",children:"Wrap-up"}),"\n",(0,s.jsx)(n.p,{children:"This simple neural network successfully learned to add two numbers. The final accuracy on the test set is 100%, indicating that the model did a good job at predicting the sum of the test input pairs. This example demonstrates the basic workflow of creating and training a neural network in PyTorch."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},6692:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/all-epocs-7252db7356f8a74fa5d43b734ad25c99.png"},2721:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/last-100-epocs-522673c110b37b241285c1e9cf2259cd.png"},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var s=t(6540);const a={},r=s.createContext(a);function i(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);